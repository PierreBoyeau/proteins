{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from word2vec import classification_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "model_path = '/home/pierre/riken/word2vec/prot_vec_model.model'\n",
    "data_path = '/home/pierre/riken/data/riken_data/complete_from_xlsx.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As input we want list containing indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep='\\t')\n",
    "df.loc[:, 'seq_len'] = df.sequences.apply(len)\n",
    "df = df.loc[df.seq_len >= 50, :]\n",
    "\n",
    "X, y = df['sequences'].values, df['is_allergenic'].values\n",
    "X_tokens = classification_tools.ProteinTokenizer(token_size=3).transform(X)\n",
    "lenghts = df.seq_len.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/.conda/envs/python3/lib/python3.5/site-packages/Bio/Seq.py:163: BiopythonWarning: Biopython Seq objects now use string comparison. Older versions of Biopython used object comparison. During this transition, please use hash(id(my_seq)) or my_dict[id(my_seq)] if you want the old behaviour, or use hash(str(my_seq)) or my_dict[str(my_seq)] for the new string hashing behaviour.\n",
      "  \"the new string hashing behaviour.\", BiopythonWarning)\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.word2vec.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_idx = {'_UNKNOWN': 0}\n",
    "embeddings = []\n",
    "\n",
    "for sentence in X_tokens:\n",
    "    sentence_embed = []\n",
    "    for token in sentence:\n",
    "        if token not in model.wv:\n",
    "            token = '_UNKNOWN'\n",
    "        if token not in labels_idx:\n",
    "            labels_idx[token] = len(labels_idx)\n",
    "        idx = labels_idx[token]\n",
    "        sentence_embed.append(idx)\n",
    "    embeddings.append(sentence_embed)\n",
    "\n",
    "matrix_embeddings = np.zeros((len(labels_idx), 100))\n",
    "labels_idx.pop('_UNKNOWN', None)\n",
    "matrix_embeddings[1:] = model.wv[labels_idx.keys()]\n",
    "\n",
    "embeddings = pad_sequences(embeddings, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(embeddings, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "import keras.metrics\n",
    "\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_nn = Sequential()\n",
    "lstm_nn.add(Embedding(len(matrix_embeddings), output_dim=100, weights=[matrix_embeddings], trainable=False))\n",
    "lstm_nn.add(LSTM(10, return_sequences=True))\n",
    "lstm_nn.add(Dropout(0.5))\n",
    "lstm_nn.add(LSTM(10))\n",
    "lstm_nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_nn.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 100)         817400    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, None, 10)          4440      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 822,691\n",
      "Trainable params: 5,291\n",
      "Non-trainable params: 817,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8729 samples, validate on 3741 samples\n",
      "Epoch 1/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.5075 - acc: 0.8443 - val_loss: 0.4379 - val_acc: 0.8415\n",
      "Epoch 2/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4328 - acc: 0.8443 - val_loss: 0.4371 - val_acc: 0.8415\n",
      "Epoch 3/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4330 - acc: 0.8443 - val_loss: 0.4370 - val_acc: 0.8415\n",
      "Epoch 4/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.4330 - acc: 0.8443 - val_loss: 0.4368 - val_acc: 0.8415\n",
      "Epoch 5/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4326 - acc: 0.8443 - val_loss: 0.4367 - val_acc: 0.8415\n",
      "Epoch 6/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4320 - acc: 0.8443 - val_loss: 0.4365 - val_acc: 0.8415\n",
      "Epoch 7/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4323 - acc: 0.8443 - val_loss: 0.4361 - val_acc: 0.8415\n",
      "Epoch 8/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4316 - acc: 0.8443 - val_loss: 0.4356 - val_acc: 0.8415\n",
      "Epoch 9/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4308 - acc: 0.8443 - val_loss: 0.4347 - val_acc: 0.8415\n",
      "Epoch 10/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4291 - acc: 0.8443 - val_loss: 0.4337 - val_acc: 0.8415\n",
      "Epoch 11/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4268 - acc: 0.8443 - val_loss: 0.4274 - val_acc: 0.8415\n",
      "Epoch 12/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4131 - acc: 0.8443 - val_loss: 0.4169 - val_acc: 0.8415\n",
      "Epoch 13/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.4088 - acc: 0.8443 - val_loss: 0.4105 - val_acc: 0.8415\n",
      "Epoch 14/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3967 - acc: 0.8443 - val_loss: 0.4028 - val_acc: 0.8415\n",
      "Epoch 15/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3941 - acc: 0.8444 - val_loss: 0.3985 - val_acc: 0.8415\n",
      "Epoch 16/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3877 - acc: 0.8455 - val_loss: 0.4006 - val_acc: 0.8415\n",
      "Epoch 17/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3855 - acc: 0.8474 - val_loss: 0.3911 - val_acc: 0.8476\n",
      "Epoch 18/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3809 - acc: 0.8520 - val_loss: 0.3910 - val_acc: 0.8420\n",
      "Epoch 19/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3779 - acc: 0.8518 - val_loss: 0.3937 - val_acc: 0.8551\n",
      "Epoch 20/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3779 - acc: 0.8539 - val_loss: 0.3813 - val_acc: 0.8549\n",
      "Epoch 21/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.3699 - acc: 0.8598 - val_loss: 0.3767 - val_acc: 0.8650\n",
      "Epoch 22/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3668 - acc: 0.8622 - val_loss: 0.3863 - val_acc: 0.8532\n",
      "Epoch 23/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3638 - acc: 0.8656 - val_loss: 0.3893 - val_acc: 0.8426\n",
      "Epoch 24/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3615 - acc: 0.8675 - val_loss: 0.3676 - val_acc: 0.8690\n",
      "Epoch 25/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3578 - acc: 0.8670 - val_loss: 0.3653 - val_acc: 0.8682\n",
      "Epoch 26/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3536 - acc: 0.8709 - val_loss: 0.3775 - val_acc: 0.8663\n",
      "Epoch 27/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3506 - acc: 0.8744 - val_loss: 0.3638 - val_acc: 0.8669\n",
      "Epoch 28/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3477 - acc: 0.8722 - val_loss: 0.3677 - val_acc: 0.8712\n",
      "Epoch 29/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3413 - acc: 0.8789 - val_loss: 0.3612 - val_acc: 0.8744\n",
      "Epoch 30/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3425 - acc: 0.8758 - val_loss: 0.3625 - val_acc: 0.8736\n",
      "Epoch 31/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.3377 - acc: 0.8805 - val_loss: 0.3514 - val_acc: 0.8749\n",
      "Epoch 32/49\n",
      "8729/8729 [==============================] - 46s 5ms/step - loss: 0.3366 - acc: 0.8796 - val_loss: 0.3587 - val_acc: 0.8717\n",
      "Epoch 33/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.3335 - acc: 0.8836 - val_loss: 0.3476 - val_acc: 0.8784\n",
      "Epoch 34/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.3283 - acc: 0.8861 - val_loss: 0.3450 - val_acc: 0.8800\n",
      "Epoch 35/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3266 - acc: 0.8854 - val_loss: 0.3430 - val_acc: 0.8802\n",
      "Epoch 36/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3231 - acc: 0.8880 - val_loss: 0.3441 - val_acc: 0.8802\n",
      "Epoch 37/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.3372 - acc: 0.8809 - val_loss: 0.3426 - val_acc: 0.8802\n",
      "Epoch 38/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3196 - acc: 0.8888 - val_loss: 0.3402 - val_acc: 0.8824\n",
      "Epoch 39/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.3186 - acc: 0.8892 - val_loss: 0.3375 - val_acc: 0.8827\n",
      "Epoch 40/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3152 - acc: 0.8907 - val_loss: 0.3363 - val_acc: 0.8832\n",
      "Epoch 41/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3121 - acc: 0.8928 - val_loss: 0.3461 - val_acc: 0.8786\n",
      "Epoch 42/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3110 - acc: 0.8927 - val_loss: 0.3382 - val_acc: 0.8829\n",
      "Epoch 43/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3065 - acc: 0.8977 - val_loss: 0.3404 - val_acc: 0.8808\n",
      "Epoch 44/49\n",
      "8729/8729 [==============================] - 48s 5ms/step - loss: 0.3076 - acc: 0.8977 - val_loss: 0.3353 - val_acc: 0.8859\n",
      "Epoch 45/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3050 - acc: 0.8972 - val_loss: 0.3644 - val_acc: 0.8717\n",
      "Epoch 46/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3078 - acc: 0.8933 - val_loss: 0.3302 - val_acc: 0.8840\n",
      "Epoch 47/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.3017 - acc: 0.8987 - val_loss: 0.3325 - val_acc: 0.8829\n",
      "Epoch 48/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.2973 - acc: 0.9006 - val_loss: 0.3514 - val_acc: 0.8765\n",
      "Epoch 49/49\n",
      "8729/8729 [==============================] - 47s 5ms/step - loss: 0.2943 - acc: 0.9017 - val_loss: 0.3307 - val_acc: 0.8861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb708ec7d30>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb = TensorBoard(log_dir='./logs4')\n",
    "ckpt = ModelCheckpoint(filepath='./logs4')\n",
    "lstm_nn.fit(Xtrain, ytrain, batch_size=128, epochs=49, validation_data=(Xtest, ytest), \n",
    "          callbacks=[tb]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'logs': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = lstm_nn.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.89      0.98      0.94      3148\n",
      "       True       0.81      0.37      0.51       593\n",
      "\n",
      "avg / total       0.88      0.89      0.87      3741\n",
      "\n",
      "ROC AUC SCORE:  0.7809005316151372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(classification_report(ytest, ypred >= 0.5))\n",
    "\n",
    "print('ROC AUC SCORE: ', roc_auc_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
